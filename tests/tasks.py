import numpy as np
import sklearn.utils
from sklearn.metrics import roc_auc_score
import gzip
import os.path
import hashlib
from pipeline import FeatureConcatPipeline


# flatten data down to 2 dimensions for putting through a classifier
# supports input shapes:
#   (num_segments, num_features)
#   (num_segments, num_windows, num_features)
#   (num_segments, num_windows, num_channels, num_features)
def flatten(data):
    if data.ndim == 2:
        return data
    if not data.ndim >= 3:
        print('data shape', data.shape)
        assert data.ndim >= 3
    s = data.shape
    out = data.reshape((np.product(s[0:2]), np.product(s[2:])))

    return out


# Load data for a given pipeline. This wraps load_data_mp to also provide FeatureConcatPipeline support.
# See load_data_mp for description of check_only and meta_only parameters.
def load_pipeline_data(settings, target, data_type, pipeline, check_only, quiet=False, meta_only=False):
    if check_only:
        return np.alltrue([load_data_mp(settings, target, data_type, p, check_only=True, quiet=quiet)
                           for p in pipeline.get_pipelines()])

    if isinstance(pipeline, FeatureConcatPipeline):
        data = []
        meta = None
        num_features = 0

        for p in pipeline.get_pipelines():
            _data, _meta = load_data_mp(settings, target, data_type, p, quiet=quiet, meta_only=meta_only)
            data.append(_data)
            if meta is None:
                meta = _meta
            for k in meta.keys():
                if k == 'X_shape':
                    assert meta[k][:-1] == _meta[k][:-1]
                    num_features += _meta[k][-1]
                elif isinstance(_meta[k], np.ndarray):
                    assert np.alltrue(meta[k] == _meta[k])
                else:
                    assert meta[k] == _meta[k]

        d0 = data[0]
        if meta_only:
            data = None
            # combine shapes
            meta['X_shape'] = list(meta['X_shape'][:-1]) + [num_features]
        else:
            for d in data[1:]:
                if d0.ndim != d.ndim:
                    print pipeline.get_name()
                    print 'd0', d0.shape, 'other', d.shape
                    assert d0.ndim == d.ndim
                assert d0.shape[:-1] == d.shape[:-1]
            data = np.concatenate(data, axis=data[0].ndim-1)
    else:
        data, meta = load_data_mp(settings, target, data_type, pipeline, quiet=quiet, meta_only=meta_only)

    return data, meta


# Load training data, this loads the preictal and interictal pipeline data, optionally separates the
# data into training set and cross-validation set, and generates labels.
#
# strategy: cross-validation strategy, see LegacyStrategy() and KFoldStrategy()
# cv_fold_number: None to specify no cross-validation set for when making a submission,
#                 otherwise a number generated by the cross-validation strategy.
def load_training_data(settings, target, pipeline, check_only, strategy=None, cv_fold_number=None, quiet=False):
    cv = cv_fold_number is not None
    if check_only:
        return load_pipeline_data(settings, target, 'preictal', pipeline, check_only=True, quiet=quiet) or \
               load_pipeline_data(settings, target, 'interictal', pipeline, check_only=True, quiet=quiet)

    preictal, preictal_meta = load_pipeline_data(settings, target, 'preictal', pipeline, check_only=False, quiet=quiet)
    interictal, interictal_meta = load_pipeline_data(settings, target, 'interictal', pipeline, check_only=False, quiet=quiet)

    total_segments = preictal_meta.num_segments + interictal_meta.num_segments
    # print 'total_segments', total_segments

    if not quiet: print 'Preparing data ...',
    start = time.get_seconds()

    def make_fold(preictal_X_train, preictal_X_cv, interictal_X_train, interictal_X_cv):
        num_train_segments = preictal_X_train.shape[0] + interictal_X_train.shape[0]
        num_cv_segments = preictal_X_cv.shape[0] + interictal_X_cv.shape[0]
        assert (num_train_segments + num_cv_segments) == total_segments

        flattened_preictal_X_train = flatten(preictal_X_train)
        flattened_interictal_X_train = flatten(interictal_X_train)
        flattened_preictal_X_cv = flatten(preictal_X_cv) if cv else np.empty((0,))
        flattened_interictal_X_cv = flatten(interictal_X_cv) if cv else np.empty((0,))

        X_train = np.concatenate((flattened_preictal_X_train, flattened_interictal_X_train), axis=0)
        X_cv = np.concatenate((flattened_preictal_X_cv, flattened_interictal_X_cv), axis=0)

        preictal_y_train = np.ones((flattened_preictal_X_train.shape[0],))
        preictal_y_cv = np.ones((preictal_X_cv.shape[0],))
        interictal_y_train = np.zeros((flattened_interictal_X_train.shape[0],))
        interictal_y_cv = np.zeros((interictal_X_cv.shape[0],))

        y_train = np.concatenate((preictal_y_train, interictal_y_train), axis=0)
        y_cv = np.concatenate((preictal_y_cv, interictal_y_cv), axis=0)

        X_train, y_train = sklearn.utils.shuffle(X_train, y_train, random_state=0)

        return jsdict({
            'X_train': X_train,
            'y_train': y_train,
            'X_cv': X_cv,
            'y_cv': y_cv,
            'num_train_segments': num_train_segments,
            'num_cv_segments': num_cv_segments
        })

    if cv:
        preictal_X_train, preictal_X_cv = strategy.split_train_cv(preictal, preictal_meta, cv_fold_number)
        interictal_X_train, interictal_X_cv = strategy.split_train_cv(interictal, interictal_meta, cv_fold_number, interictal=True)
        data = make_fold(preictal_X_train, preictal_X_cv, interictal_X_train, interictal_X_cv)
    else:
        preictal_X_train = preictal
        preictal_X_cv = np.empty((0,))
        interictal_X_train = interictal
        interictal_X_cv = np.empty((0,))
        data = make_fold(preictal_X_train, preictal_X_cv, interictal_X_train, interictal_X_cv)

    if not quiet: print '%ds' % (time.get_seconds() - start)

    if not quiet: print 'X_train', data.X_train.shape, 'y_train', data.y_train.shape, 'X_cv', data.X_cv.shape, 'y_cv', data.y_cv.shape

    return data


# Load the test data for a given pipeline
def load_test_data(settings, target, pipeline, quiet=False):
    test, meta = load_pipeline_data(settings, target, 'test', pipeline, check_only=False, quiet=quiet)
    X_test = flatten(test)
    if not quiet: print 'X_test', test.shape, 'num_segments', meta.num_segments
    return jsdict({
        'X_test': X_test,
        'num_segments': meta.num_segments
    })


# Train a classifier
def train(classifier, training_data, quiet=False):
    X_train = training_data.X_train
    y_train = training_data.y_train
    if not quiet: print 'Training ...',
    start = time.get_seconds()
    classifier.fit(X_train, y_train)
    if not quiet: print '%ds' % (time.get_seconds() - start)


# Make predictions, and then combine the N predictions if using windows using mean and median.
# Returns (mean_predictions, median_predictions, raw_predictions)
def make_predictions(classifier, X, num_segments):
    predictions = classifier.predict_proba(X)[:, 1]
    split_data = np.split(predictions, num_segments)
    return to_np_array([np.mean(ps) for ps in split_data]), to_np_array([np.median(ps) for ps in split_data]), predictions


# Save the output of function fn to os.path.join(*paths) if it doesn't exist on disk,
# otherwise load the data from disk. Note that this changes the current working directory
# in order to deal with too-big filenames generated by a large number of concatenated
# features in FeatureConcatPipeline.
def memoize(fn, paths):
    cwd = os.getcwd()

    def change_to_target_dir():
        for dir in paths[:-1]:
            try:
                os.mkdir(dir)
            except OSError as e:
                pass
            os.chdir(dir)

    change_to_target_dir()
    filename = paths[-1]
    if os.path.exists(filename):
        data = hdf5.read(filename)
        os.chdir(cwd)
        return data

    os.chdir(cwd)
    data = fn()
    change_to_target_dir()
    tmp = '%s.pid.%d.tmp' % (filename, os.getpid())
    hdf5.write(tmp, data)
    os.rename(tmp, filename)
    os.chdir(cwd)

    return jsdict(data)


# Fast process-if-not-yet-processed method for training data
def check_training_data_loaded(settings, target, pipeline, quiet=False):
    if not load_pipeline_data(settings, target, 'preictal', pipeline, check_only=True, quiet=quiet):
        load_pipeline_data(settings, target, 'preictal', pipeline, check_only=False, quiet=quiet)
    if not load_pipeline_data(settings, target, 'interictal', pipeline, check_only=True, quiet=quiet):
        load_pipeline_data(settings, target, 'interictal', pipeline, check_only=False, quiet=quiet)


# Fast process-if-not-yet-processed method for test data
def check_test_data_loaded(settings, target, pipeline, quiet=False):
    if not load_pipeline_data(settings, target, 'test', pipeline, check_only=True, quiet=quiet):
        load_pipeline_data(settings, target, 'test', pipeline, check_only=False, quiet=quiet)


# Represent a feature_mask e.g. [1,0,0,1,1,1,0] as binary.
def calc_feature_mask_bigint(mask):
    mask = [int(x) for x in mask]
    out = 0
    for i, x in enumerate(mask):
        out += x << i
    return out


# Represent a feature_mask e.g. [1,0,0,1,1,1,0] as string by concatenating
# md5 and sha1. Should be unique enough. Used to provide a short-name for
# saving data to disk. Otherwise the filename would be way too long.
def calc_feature_mask_string(mask):
    if mask is None:
        return None

    out = calc_feature_mask_bigint(mask)

    hex_str = hex(out)
    md5 = hashlib.md5(hex_str).hexdigest()
    sha1 = hashlib.sha1(hex_str).hexdigest()
    return md5 + sha1


# Calculate cross-validation score for a single cv fold.
def cross_val_score_for_one_fold(settings, target, pipeline, classifier, classifier_name, fold, strategy, feature_mask=None, progress_str=None, quiet=False):
    def process():

        data = load_training_data(settings, target, pipeline, strategy=strategy, cv_fold_number=fold, check_only=False, quiet=quiet)

        if feature_mask is not None:
            s = [slice(None),] * data.X_train.ndim
            s[-1] = np.where(np.array(feature_mask) == True)[0]
            data['X_train'] = data.X_train[s]
            data['X_cv'] = data.X_cv[s]
            if not quiet: print ' feature mask', 'X_train', data.X_train.shape, 'y_train', data.y_train.shape, 'X_cv', data.X_cv.shape, 'y_cv', data.y_cv.shape

        train(classifier, data, quiet=quiet)
        if not quiet: print "Making predictions...",
        timer = time.Timer()
        mean_predictions, median_predictions, raw_predictions = make_predictions(classifier, data.X_cv, data.num_cv_segments)
        if not quiet: print timer.pretty_str()

        mean_score = roc_auc_score(data.y_cv, mean_predictions)
        median_score = roc_auc_score(data.y_cv, median_predictions)

        return jsdict({
            'mean_score': mean_score,
            'median_score': median_score,
            'mean_predictions': mean_predictions,
            'median_predictions': median_predictions,
            'y_cv': data.y_cv
        })

    feature_mask_string = calc_feature_mask_string(feature_mask)
    fm_path = [feature_mask_string] if feature_mask_string is not None else []
    paths = [settings.cache_dir, target, classifier_name] + pipeline.get_names() + fm_path + ['cv_%s_fold%d.hdf5' % (strategy.get_name(), fold)]

    if progress_str is not None:
        print 'Running', progress_str, 'fold %d' % fold
    return memoize(process, paths)


# Calculate the average cross-validation score across N folds.
#
# pool: Optional multi-processing pool to use to schedule the folds, otherwise folds
#       will be processed one-by-one
# strategy: cross-validation strategy, see LegacyStrategy() and KFoldStrategy()
# feature_mask: The feature_mask to apply before training.
# progress_str: helper string for printing progress inside multiprocessing pool
# return_data: returns full result if True, otherwise simply processes the folds without doing an consolidation work
def cross_validation_score(settings, target, pipeline, classifier, classifier_name, strategy=None, pool=None, progress_str=None, feature_mask=None, return_data=True, quiet=False):
    if strategy is None:
        strategy = KFoldStrategy()

    if feature_mask is not None and np.count_nonzero(feature_mask) == len(feature_mask):
        feature_mask = None

    _, preictal_meta = load_pipeline_data(settings, target, 'preictal', pipeline, check_only=False, quiet=quiet, meta_only=True)
    cv_folds = strategy.get_folds(preictal_meta)

    if pool is not None:
        results = [pool.apply_async(cross_val_score_for_one_fold, [settings, target, pipeline, classifier, classifier_name, fold],
                                    {'strategy': strategy, 'feature_mask': feature_mask, 'progress_str': progress_str, 'quiet': quiet})
                   for fold in cv_folds]
        if return_data:
            out = [r.get() for r in results]
    else:
        out = [cross_val_score_for_one_fold(settings, target, pipeline, classifier, classifier_name, strategy=strategy,
                                            fold=fold, feature_mask=feature_mask, progress_str=progress_str, quiet=quiet) for fold in cv_folds]

    if return_data:
        mean_scores = [d.mean_score for d in out]
        median_scores = [d.median_score for d in out]
        mean_predictions = [d.mean_predictions for d in out]
        median_predictions = [d.median_predictions for d in out]
        y_cvs = [d.y_cv for d in out]

        return jsdict({
            'mean_score': np.mean(mean_scores),
            'median_score': np.mean(median_scores),
            'mean_scores': np.array(mean_scores),
            'median_scores': np.array(median_scores),
            'mean_predictions': mean_predictions,
            'median_predictions': median_predictions,
            'y_cvs': y_cvs
        })


# Make submission predictions for a given pipeline and classifier.
#
# feature_mask: The feature_mask to apply before training.
# progress_str: helper string for printing progress inside multiprocessing pool
def make_submission_predictions(settings, target, pipeline, classifier, classifier_name, feature_mask=None, quiet=False, progress_str=None):
    if progress_str is not None:
        print 'Running', progress_str

    feature_mask_string = calc_feature_mask_string(feature_mask)

    def process():
        data = load_training_data(settings, target, pipeline, check_only=False, quiet=quiet)

        if feature_mask is not None:
            s = [slice(None),] * data.X_train.ndim
            s[-1] = np.where(np.array(feature_mask) == True)[0]
            data['X_train'] = data.X_train[s]
            if not quiet: print 'Feature mask', 'X_train', data.X_train.shape

        train(classifier, data, quiet=quiet)
        train_predictions = classifier.predict_proba(data.X_train)[:, 1]
        y_train = data.y_train
        del data

        data = load_test_data(settings, target, pipeline, quiet=quiet)

        if feature_mask is not None:
            s = [slice(None),] * data.X_test.ndim
            s[-1] = np.where(np.array(feature_mask) == True)[0]
            data['X_test'] = data.X_test[s]
            if not quiet: print 'Feature mask', 'X_test', data.X_test.shape

        predictions = make_predictions(classifier, data.X_test, data.num_segments)
        predictions_mean, predictions_median, test_predictions = predictions

        return {
            'mean_predictions': predictions_mean,
            'median_predictions': predictions_median,
            'train_predictions': train_predictions,
            'y_train': y_train,
            'test_predictions': test_predictions,
            'num_segments': data.num_segments
        }

    fm_path = [feature_mask_string] if feature_mask_string is not None else []
    paths = [settings.cache_dir, target, classifier_name] + pipeline.get_names() + fm_path + ['predictions.hdf5']
    return memoize(process, paths)


# Convert predictions into csv submission format
def make_csv_for_target_predictions(target, predictions):
    return ['%s_test_segment_%.4d.mat,%.10f' % (target, i+1, p) for i, p in enumerate(predictions)]


# Wrapper to return both mean and median-combined predictions in csv submission format
def make_submission_csv(settings, target, pipeline, classifier, classifier_name):
    data = make_submission_predictions(settings, target, pipeline, classifier, classifier_name, quiet=True)

    csv_mean = make_csv_for_target_predictions(target, data.mean_predictions)
    csv_median = make_csv_for_target_predictions(target, data.median_predictions)

    return csv_mean, csv_median


# Write a submission file given the guesses either as a list of strings or already as the final string.
# Filename is generated as submission%d.csv.gz with companion submission%d.txt where the number is
# auto-increment given existing files in the submission directory. The companion txt file provides info
# about what was used to generate that submission.
def write_submission_file(settings, guesses, name, pipeline, classifier_name, targets_and_pipelines=None, target_pipelines=None):
    guesses = '\n'.join(guesses) if isinstance(guesses, list) else guesses
    id = 0
    done = False
    while not done:
        try:
            filename = os.path.join(settings.submission_dir, 'submission%d.csv.gz' % id)
            # make the file to 'take it'
            fd = os.open(filename, os.O_CREAT | os.O_EXCL | os.O_WRONLY, 0644)
            os.close(fd)

            f = gzip.open(filename, 'wb')
            f.write(guesses)
            f.close()

            print 'wrote', filename

            filename = os.path.join(settings.submission_dir, 'submission%d.txt' % id)
            with open(filename, 'w') as f:
                print >>f, classifier_name
                print >>f, name
                if target_pipelines is not None:
                    for target in sorted(target_pipelines.keys()):
                        pipeline = target_pipelines[target]
                        print >>f, target
                        print >>f, pipeline.get_name()
                if targets_and_pipelines is not None:
                    for target, pipeline, feature_masks, _, _ in targets_and_pipelines:
                        print >>f, target
                        print >>f, 'FEATURE MASKS'
                        print >>f, '\n'.join(pipeline.get_names())
                        for i, mask in enumerate(feature_masks):
                            print >>f, 'Mask %d' % i
                            print >>f, mask
                else:
                    for p_name in pipeline.get_names():
                        print >>f, p_name
            print 'wrote', filename

            done = True

        except OSError as e:
            id += 1